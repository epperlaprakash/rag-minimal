Minimal RAG Prototype

A tiny Retrieval-Augmented Generation (RAG) demo in Python.
It loads a few .txt files, splits them into chunks, builds a vector index (FAISS), retrieves the most relevant chunks, and asks an LLM (Ollama by default) to answer using only that context.

**Features:**
    Loads & embeds 3+ text files from data/
    Overlapping chunking
    Vector search with FAISS
    Simple CLI (type a question ‚Üí get an answer)
    Shows sources with score % + snippet
    ‚ÄúI don‚Äôt know‚Äù guardrail when context is missing
    LLM: Ollama (Llama 3) by default; optional OpenAI API

üìÇ **Project Structure**
rag-minimal/
‚îú‚îÄ data/            # .txt knowledge files
‚îÇ  ‚îú‚îÄ animals.txt
‚îÇ  ‚îú‚îÄ planets.txt
‚îÇ  ‚îî‚îÄ history.txt
‚îú‚îÄ artifacts/       # auto-saved index & metadata (created on first run)
‚îú‚îÄ rag_cli.py       # main script
‚îú‚îÄ README.md
‚îú‚îÄ requirements.txt
‚îî‚îÄ .gitignore


**Requirements:**
    Python 3.10+
    Python packages: 

    ```bash 
    pip install -r requirements.txt
    ```

    One LLM path:
        Ollama (local, free) ‚Äî recommended, or
        OpenAI API key (no local install)

requirements.txt

sentence-transformers
faiss-cpu
numpy
# openai  # optional; only if you want to use OpenAI

üß† **Choose your LLM**
Option A ‚Äî Ollama (local, free)
Download & install: https://ollama.com/download
Open the Ollama app once so the service starts.
In Terminal, pull a model:

```bash
ollama pull llama3
```

Quick test:
```bash
ollama run llama3 "hello"
```


# Option B ‚Äî OpenAI (no local install)

Create an API key: https://platform.openai.com
 ‚Üí View API keys ‚Üí Create new key

Set the key in your shell:

    macOS/Linux:

    ```bash
    export OPENAI_API_KEY="sk-..."
    ```


    Windows PowerShell:

    ```bash
    $env:OPENAI_API_KEY="sk-..."
    ```


The script will auto-use OpenAI if OPENAI_API_KEY is set; otherwise it uses Ollama.

‚öôÔ∏è **Setup**
macOS / Linux

```bash 
cd rag-minimal
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

Windows (PowerShell)

```bash
cd rag-minimal
py -m venv .venv
.\.venv\Scripts\activate
pip install -r requirements.txt
# If activation is blocked:
# Set-ExecutionPolicy -Scope Process -ExecutionPolicy Bypass
```

Put at least 3 .txt files in data/ (the repo includes examples).

‚ñ∂Ô∏è Run
```bash
python3 rag_cli.py 
```    # macOS/Linux
# or
```bash
python rag_cli.py      # Windows
```

You‚Äôll see:

== Minimal RAG Demo ==
RAG is ready. Type your question (or 'exit').
Try questions like:
  ‚Ä¢ What is the fastest land animal?
  ‚Ä¢ Which planet has the Great Red Spot?
  ‚Ä¢ What did Gutenberg invent?
> 

Example
> What is the fastest land animal?

--- Answer ---
According to the provided documents, the fastest land animal is the cheetah... [animals.txt].

Sources:
  1. [animals.txt]  (67.4%)  Cheetahs are the fastest land animals, capable of short bursts up to about 100‚Äì120 km/h‚Ä¶
  2. [planets.txt]  (18.9%)  Olympus Mons, is on Mars. Jupiter is the largest planet, a gas giant with strong bands‚Ä¶

‚ÄúI don‚Äôt know‚Äù behavior
> Who is the president of the United States?

--- Answer ---
I don't know based on the provided documents. The context only contains information about planets and animals‚Ä¶

Sources:
  1. [planets.txt]  (15.7%)  Olympus Mons, is on Mars‚Ä¶
  2. [animals.txt]  (6.4%)   Cheetahs are the fastest land animals‚Ä¶

# Rebuild the index if and when you change the .txt files in data folder. simply delete the files in artifacts folder and then rerun the rag_cli.py


**üß™ Tools / Models Used**
Embeddings: sentence-transformers/all-MiniLM-L6-v2
Vector store: FAISS (inner product on normalized vectors = cosine)
LLM: Ollama Llama 3 by default (or OpenAI if OPENAI_API_KEY is set)
Language: Python 3.11

‚è±Ô∏è **Time Spent**
~4‚Äì5 hours (setup, learning, coding, testing).